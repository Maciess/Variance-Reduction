---
title: "Project 2 Monte Carlo"
author: "Maciej Szczutko"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, warning = F)
library(MASS)
```

```{r params}

r <- 0.05
sigma <- 0.25
mu_star <- r - sigma^2
s_0 <- 100
K <- 100

source("helpers.R")

```

# Problem description

We are interested in estimating the following (called an option, with discounted payoff at time 1) with price given by the formula

$$
I=e^{-r} E\left(A_n-K\right)_{+},
$$
where
$$
A_n=\frac{1}{n} \sum_{i=1}^n S(i / n), 
$$
and 
$$
S(t)=S(0) \exp \left(\mu^* t+\sigma B(t)\right), \quad 0 \leq t \leq T
$$
where $B(t)(0 \leq t \leq T)$ is Brownian motion.


## European and Asian option

In the case n = 1, this is called a European call option; otherwise, it is called an Asian call option.


## Used methods

  1. Crude Monte Carlo estimator
  2. Stratified estimator
  3. Anthithetic variables
  4. Control variates
  
  
## Monte Carlo description

First we need to generate Brownian Motion $n$ points, equally spaced sample on $[0,1]$. We will use the fact that $\mathbf{B}=(B(1 / n), B(2 / n), \ldots, B(1))$ is a multivariate normal random variable $\mathcal{N}(\mathbf{0}, \boldsymbol{\Sigma})$ with the covariance matrix

$$
\boldsymbol{\Sigma}(i, j)=\frac{1}{n} \min (i, j).
$$




```{r monte-carlo}

n <- 1

set.seed(42)

numOfReplicate <- 10000
monteCarloReplicates <- replicate(numOfReplicate, {
gmbSample <- GetGeometricBrownianSample(s_0, r, sigma, n);
A <- mean(gmbSample);
I <- exp(-r) * max(A-K, 0)},
simplify = "vector")
monteCarloEstimator <- mean(monteCarloReplicates)


```


```{r simulation-for-larger-n}
monteCarloSimulation <- function(n, numOfReplicate, s_0, r, sigma, K)
{
  monteCarloReplicates <- replicate(numOfReplicate, {
    gmbSample <- GetGeometricBrownianSample(s_0, r, sigma, n)
    
    A <- mean(gmbSample)
    
    I <- exp(-r) * max(A - K, 0)
  },
  simplify = "vector")
  monteCarloReplicates
}

```

```{r power-of-two, cache=TRUE}

set.seed(42)
indexVector <- 2^(0:5)

R <- 10000
mcResults <- as.data.frame(sapply(indexVector, function(i) {monteCarloSimulation(n=i, numOfReplicate = R, s_0, r, sigma, K)}))
names(mcResults) <- indexVector


variance.mc <- apply(mcResults, 2, var) / R



```

```{r}
library(kableExtra)
mcEstimators <- colMeans(mcResults)
kbl(as.data.frame(t(mcEstimators)), booktabs = T, caption = paste0("MC estimators values for R=", R), digits = 3) %>%
  kable_classic()

mc.estimators.and.var <- cbind(data.frame(mcEstimators), data.frame(variance.mc))
```




```{r plot-for-different-n, fig.dim=c(6, 2.5)}
library(ggplot2)
library(tidyr)


df_long <- pivot_longer(mcResults, cols = everything(), names_to = "Variable", values_to = "Value")
df_long$Variable <- factor(df_long$Variable, levels = names(mcResults))

# Create the boxplot
ggplot(df_long, aes(x = Variable, y = Value)) +
  geom_boxplot(fill = "skyblue", color = "black") +
  labs(title = "Boxplot of Each Column", x = "Columns", y = "Values") +
  theme_minimal()
```


```{r plots-for-MC, include=F}
#to do not needed
data <- data.frame(
  Value = c(monteCarloReplicates, monteCarloReplicates),
  Group = rep(c("Crude Monte Carlo estimator", "Place holder"), each = 100)
)

ggplot(data, aes(x = Group, y = Value, fill = Group)) +
  geom_boxplot() +
  labs(title = paste("CMCE vs ... for European Option with R =", numOfReplicate),
       x = "Groups",
       y = "Values") +
  theme_minimal() +
  scale_fill_manual(values = c("skyblue", "orange"))


```

## Theoretical calculation for European option using Black-Scholes formula

We can compare the simulation results for European option with theoretical calculation. The formula is

$$
E(S(1)-K)_{+}=S(0) \Phi\left(d_1\right)-K e^{-r} \Phi\left(d_2\right),
$$

where

$$
d_1=\frac{1}{\sigma}\left[\log \left(\frac{S(0)}{K}\right)+r+\frac{\sigma^2}{2}\right],
$$
and

$$
d_2=d_1-\sigma .
$$


```{r black-scholes}
d1 <- (1 / sigma) * (log(s_0 / K) + r + (sigma ^ 2 / 2))
d2 <- d1 - sigma
blackScholes <- (s_0 * pnorm(d1)) - (K * exp(-r) * pnorm(d2))
```
Using our setup parameters we obtain value $`r blackScholes`$. It is align with the MC simulation result. Even using $R=`r R`$ replication we have quite bias simulation. 


### Observation 

For European option (instant pay off ?) we have higher expected value but with higher variability. For Asian option the variability is a bit lower but the estimated payoff is also lower. The value of pay off seems converge to some value as $n$ increase. I would bet for some constants (e.g. 6/7) but I haven't perform simulation for extremely large $n$ to confirm this.

## Stratified sampling 


Stratified sampling is a sampling technique in which a population is divided into smaller, homogeneous groups called strata based on shared characteristics (e.g., age, gender, income). In this example the strata is determined by the ellipses. Then, a random sample is taken from each stratum proportionally or equally to ensure all groups are represented in the final sample. This method improves accuracy and reduces sampling bias and variance. 


### The algorithm for generating sample from $N(0, \Sigma)$.

  1. Perform Cholesky decomposition: $\boldsymbol{\Sigma}=\mathbf{A A}^T$.
  2. Sample $\boldsymbol{\xi}=\left(\xi_1, \ldots, \xi_n\right)^T$, where $\xi_i \sim \mathcal{N}(0,1)$ i.i.d. Set

$$
\mathbf{X}=\left(X_1, \ldots, X_n\right)^T=\left(\frac{\xi_1}{\|\boldsymbol{\xi}\|}, \ldots, \frac{\xi_n}{\|\boldsymbol{\xi}\|}\right)^T
$$

  3. Sample $U \sim \mathcal{U}(0,1)$. Set

$$
D^2=F_{\chi_n^2}^{-1}\left(\frac{i-1}{m}+\frac{1}{m} U\right)
$$

  4. Set $\mathbf{Z}=\left(Z_1, \ldots, Z_n\right)=\left(D X_1, \ldots, D X_n\right)$.
  5. Return $\mathbf{B}^i=\mathbf{A Z}$.


```{r stratified-sample-n2}
m <- 5
n <- 2
samples_per_stratum <- 500

data_list <- lapply(1:m, function(j) {
  # Generate samples for each stratum
  samples <- replicate(samples_per_stratum, GetStratifiedMvNormFromJthStratum(n, j, m))
  df <- as.data.frame(t(samples)) # Transpose to get rows as samples
  df$j <- j # Add stratum column
  return(df)
})

# Combine all strata into a single data frame
result_df <- do.call(rbind, data_list)
colnames(result_df) <- c(paste0("V", 1:n), "j") # Name columns

```

Lets see how the algorithm works. The $\Sigma$ is the same as before for Brownian motion. 

```{r plot-strata, fig.cap="Starat viusalization", fig.dim=c(7,3)}
library(ggplot2)

ggplot(result_df, aes(x = V1, y = V2, color = as.factor(j))) +
  geom_point(size = 0.7, alpha = 0.7) +
  scale_color_discrete(name = "Stratum (j)") + # Legend title
  labs(
    title = paste0("Stratified Samples with m=", m, " and R_j=", samples_per_stratum),
    x = "V1",
    y = "V2"
  ) +
  theme_minimal() +
  theme(legend.position = "right")
```

It looks that indeed procedure works as expected. Note that our stratas are ellipses, not the circle. But the assumption that each strata has probability $\frac{1}{m}$ still holds. Or change but since we apply the same linear transformation this will not impact calculation as the factor simplify at the end.

### Estimator formulas

$$
\hat{Y}_{R_j}^j=\frac{1}{R_j} \sum_{i=1}^{R_j} Y_i^j, \quad \hat{Y}_R^{\mathrm{str}}=p_1 \hat{Y}_{R_1}^1+\ldots+p_m \hat{Y}_{R_{\mathrm{m}}}^m, 
$$


$$ \operatorname{Var} \hat{Y}_R^j=\frac{\sigma_j^2}{R_j},  \quad \operatorname{Var} \hat{Y}_R^{\mathrm{str}}=\sum_{j=1}^m p_j^2 \operatorname{Var} \hat{Y}_R^j=\sum_{j=1}^m \frac{p_j^2}{R_j} \sigma_j^2 .$$ 

### Proportional scheme for n=1. 

For stratified sampling we can use also the different number of replication for each stratum. In proportional scheme we simply take $R_j=p_jR$. For below simulation I will use $m=5$ and $R=10000$ (to be comparable with CRMC).

```{r estimator}
StratifiedSimulation <-
  function(n,
           numOfReplicatePerStrata,
           s_0,
           r,
           sigma,
           K)
  {
    R <- sum(numOfReplicatePerStrata)
    m <- length(numOfReplicatePerStrata)
    stratifiedReplicates <-
      sapply(1:m, function(j)
        replicate(numOfReplicatePerStrata[j],
                  {
                    gmbSample <- GetGmbFromStrata(s_0, r, sigma, n, j, m)
                    A <- mean(gmbSample)
                    I <- exp(-r) * max(A - K, 0)
                  }))
    stratifiedReplicates
  }



```


```{r}
set.seed(42)
R <- 10000
m <- 5
n <- 1
samples_per_stratum <- rep(R/m, m)

stratifiedTestForN1 <- StratifiedSimulation(1, samples_per_stratum, s_0, r, sigma, K)

estimator_for_n_1 <- sum(1/m * colMeans(stratifiedTestForN1))

probs <- rep(1/m, m)^2
var_estimator_for_n_1 <- sum((probs / samples_per_stratum) * apply(stratifiedTestForN1, 2, var))

mc_var_n1 <- var(mcResults[, 1]) / nrow(mcResults)

```


The estimator for $\hat{I} = `r estimator_for_n_1`$. Calculated variance is $`r var_estimator_for_n_1`$. For MC the estimator variance is $`r mc_var_n1`$. So the variance is decreased even with simple choice of strata numbers.

### Optimal choice for $n=1$.

Now we will use optimal allocation scheme. From theorem presented in the script we can take 

$$
R_j=\frac{p_j \sigma_j}{\sum_{i=1}^{m} p_i \sigma_i} R.
$$

As we don't know the variance for each stratum (or it's to complex to calculate by hand for me) we can use following procedure.

  1. Use proportional scheme to estimate $\sigma_i$.
  2. Calculate $R_j$ based using estimator for $\sigma_i$.
  3. Sample again and estimate.


```{r optional-allocation-scheme}

set.seed(42)
sigma.estimated <-  apply(stratifiedTestForN1, 2, sd)
factor <- R / sum(probs*sigma.estimated) 

r.for.optimal.scheme <- floor(probs*sigma.estimated * factor)

optima.scheme.replicate.n1 <- StratifiedSimulation(1, r.for.optimal.scheme, s_0, r, sigma, K)

estimator.for.optimal.n1 <- sum(unlist(lapply(optima.scheme.replicate.n1, mean)) * 1/m)


var <- unlist(lapply(optima.scheme.replicate.n1, var))

var.for.optimal.n1 <- sum(probs * var / r.for.optimal.scheme )

```


The estimated value for $I = `r estimator.for.optimal.n1`$. The variance is $`r var.for.optimal.n1`$. 

### $n \geq 2$ using proportional and best scheme 

Now we can calculate the estimator for higher $n$. Proportion as before. The result are presented in the table.

```{r proportional-scheme-larger-n, cache=TRUE}

proportional.scheme <- sapply(indexVector, function(i)
  {
    result.for.single.n <- StratifiedSimulation(i, samples_per_stratum, s_0, r, sigma, K)
    I.stratified.estimator <- sum(1/m * colMeans(result.for.single.n))
    probs <- rep(1/m, m)^2
    I.stratified.var <- sum((probs / samples_per_stratum) * apply(result.for.single.n, 2, var))
    
    #best allocation
    R <- sum(samples_per_stratum)
    sigma.estimated.i <- apply(result.for.single.n, 2, sd)
    factor <- R / sum(probs*sigma.estimated.i) 
    r.for.optimal.scheme <- floor(probs*sigma.estimated.i * factor)
    ##
    
    result.for.single.n.optimal <- StratifiedSimulation(i, r.for.optimal.scheme, s_0, r, sigma, K)
    
    I.stratified.estimator.optimal <-
      sum(unlist(lapply(result.for.single.n.optimal, mean)) * 1 / m)
    
    
    var.optimal <- unlist(lapply(result.for.single.n.optimal, var))
    I.stratified.var.optimal <- sum(probs * var.optimal / r.for.optimal.scheme)
    
    c(I.stratified.estimator, I.stratified.var, I.stratified.estimator.optimal, I.stratified.var.optimal)
  }
)

results.proportional <- data.frame(t(proportional.scheme))

```
```{r print-results}
colnames <- rep(c("$E(I)$", "$Var(I)$"), 3)
rownames(results.proportional) <- indexVector
kbl(cbind(mc.estimators.and.var, results.proportional), caption = "Comarison between MC and Strartified with differen allocation scheme.", escape = FALSE, digits = 5, booktabs = T, col.names = colnames)%>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  add_header_above(c(" $n$ ", "Monte Carlo" = 2, "Stratified Proportional" = 2, "Stratified Optimal" = 2))
```

### Observation

The variance techniques works according to the theory. The variance is reduced when we compare to MC. Also changing the allocation scheme give some improvement. Here the result is presented for single $R$. I made some test and for higher $R$ (e.g. $10^6$) we obtain even better improvement ratio in terms of variance. For the MC method we have smaller bias for smaller number of replication $R$. I think it also highly depends on choosing strata for simulation. In practice I would use ordinary MC method.

I haven't analyse the $m$ strata number influence here. 


## Antithetic estimator

The key idea here is to introduce dependency between elements in the sample. Analysing formula for variance for two random variables 

$$\operatorname{Var}(X+Y)=\operatorname{Var}(X)+\operatorname{Var}(Y)+2 \operatorname{Cov}(X, Y)$$

should give us the intuition: if the covariance of $X$ and $Y$ is negative the overall variance should be reduced.

The estimator formula is exactly the same as for crude MC. 

In this context we will use this method for European option. We will use  $Z_{2 i}=-Z_{2 i-1}$ where $Z_i$ is standard normal. For single generated value from brownian motion we apply following procedure.

Make the pair $\left(Z_{2 i-1}, Z_{2 i}\right)$ and compute two stock prices:

$$
\begin{aligned}
S_{2 i-1}(1) & =S(0) \exp \left(\mu +\sigma Z_{2 i-1}\right), \\
S_{2 i}(1) & =S(0) \exp \left(\mu +\sigma Z_{2 i}\right) .
\end{aligned}
$$
The for each $S_i(1)$ calculate the payoff as before.

Same number of replication is used to compare with MC method.  

```{r antihiethic-variables}

AntitheticSimulation <- function(numOfReplicate, s_0, r, sigma, K)
{
  AntitheticReplicatesOfBrownian <- replicate(numOfReplicate / 2, GetBrownianSample(1), simplify = "vector")
  merged <- numeric(numOfReplicate)
  merged[2*(1:(numOfReplicate/2))] <- -AntitheticReplicatesOfBrownian
  merged[2*(1:(numOfReplicate/2)) - 1] <- AntitheticReplicatesOfBrownian
  mu <- r - (sigma^2) / 2
  gmbSampleAnt <- s_0*exp(mu+ sigma * merged)
  I <- exp(-r) * pmax(gmbSampleAnt - K, 0)
}

set.seed(42)
AntitheticResults <- as.data.frame( AntitheticSimulation(numOfReplicate = R, s_0, r, sigma, K))

Antithethic.estimator <- colMeans(AntitheticResults)

Antithethic.var <- apply(AntitheticResults, 2, var)/R

```

The estimator value is $`r Antithethic.estimator`$. The variance is $`r Antithethic.var`$.


Observation. Variance is not reduce a lot. 


## Control Variate

This is another simple technique. The key idea here is the fact that we know the exact expected value of other random variable. Then we use following correction for crude MC estimator:

$$
\hat{Y}_R^{\mathrm{CV}}=\frac{1}{R} \sum_{i=1}^R\left(Y_j-\beta\left(X_j-\mathbb{E}[X]\right)\right),
$$
where $\beta$ is a coefficient chosen to minimize the variance of the estimator. The optimal $\beta$ is given by:

$$
\beta=\frac{\operatorname{Cov}(Y, X)}{\operatorname{Var} X}
$$
Usually we don't know the $Cov(Y,X)$. So we will use estimator from simulation.

In this context $X=B \sim N(0,1)$ will be used as control variate. Since $\mathbb{E}[B]=0$ the formula become  

$$
\hat{Y}_R^{\mathrm{CV}}=\frac{1}{R} \sum_{j=1}^R\left(Y_j-\beta X_j\right).
$$


The main difference between crude MC simulation is fact that we need to store sample from Brownian motion from each replicate.

```{r cv-simulation, cache=T}
set.seed(42)
cvSimulation <- function(numOfReplicate, s_0, r, sigma, K)
{
    cvReplicates <- replicate(numOfReplicate, {
    brownianSample <- GetBrownianSample(1)
    mu <- r - (sigma^2) / 2
    gmbSample <- s_0*exp(mu  + sigma * brownianSample)
    
    A <- mean(gmbSample)
    
    I <- exp(-r) * max(A - K, 0)
    c(I, brownianSample)
  })
  cvReplicates
}

cvResults <- data.frame(t(cvSimulation(numOfReplicate = R, s_0, r, sigma, K)))
```

```{r cv-calculate-estimator}
Y <- cvResults$X1
X <- cvResults$X2
beta <- cov(Y, X) / var(X)

cv.estimator <- mean(Y - beta * X)
cv.estimator.var <- var(Y)/length(Y)
```

Estimated value $I= `r cv.estimator`$. The variance for estimator is $`r cv.estimator.var`$. No significant improvement for European option compared to MC method. But we have lower bias comparing to previous method.

### Higher number of replication $R=1000000$

Lets check whether the difference in variance become more significant if we increase the number of replication.


```{r test-for-higher-r, cache=T}
R.higher <- as.integer(1e6)

mc.higherR <- monteCarloSimulation(n=1, numOfReplicate = R.higher, s_0, r, sigma, K)
cvResults.higherR <- data.frame(t(cvSimulation(numOfReplicate = R.higher, s_0, r, sigma, K)))
```

```{r calc-for-higher}

Y <- cvResults.higherR$X1
X <- cvResults.higherR$X2
beta <- cov(Y, X) / var(X)

cv.estimator.higherR <- mean(Y - beta * X)
cv.estimator.var.higherR <- var(Y)/length(Y)

mc.estimator.higherR <- mean(mc.higherR)
mc.var.higherR <- var(mc.higherR) / length(mc.higherR)

p <- cor(Y,X)

mc.var.n1 <- mc_var_n1
```

|   $R$    |  $Var(\hat{I}^{MC})$ | $Var(\hat{I}^{CV})$ |
|----------|-----------------------|---------------------|
|  $10000$ | `r mc.var.n1`         | `r cv.estimator.var` |
| $1000000$ | `r mc.var.higherR`   | `r cv.estimator.var.higherR`   |


That's a bit odd. According to theory variance should be reduced. From theoretical calculation

$$
\operatorname{Var}\left(\hat{Y}_R^{C V}\right)= \ldots = \operatorname{Var} \hat{Y}_R^{\mathrm{CMC}}\left(1-\rho^2\right),
$$
where $\rho$ is correlation between pay of and Brownian motion.  In this case variables should be highly correlated. Estimate for correlation is $`r p`$ (in case $R=1000000$) thus I should observe significant reduction of variance (4 times less). Please note that estimated value is close to theoretical value even with lower bias. I didn't find any issue in implementation. I don't know how to comment the results. 
